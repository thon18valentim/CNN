{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "BtdLiZBwKcyh",
        "-AjMnaikIWE5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "BtdLiZBwKcyh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93BZYA0bYtzw",
        "outputId": "88b4b5f8-01e7-4282-c577-b7ecd655affe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Add, Input, MaxPooling2D, GlobalAveragePooling2D, Dense, MaxPool2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.layers import ReLU, GlobalAvgPool2D, Dropout\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from PIL import Image\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up the testing images and the training images. Each images pass through a resize and normalization function."
      ],
      "metadata": {
        "id": "pGQWBBCkabNB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lidando com as Imagens"
      ],
      "metadata": {
        "id": "-AjMnaikIWE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imagesTesting = []\n",
        "labelsTesting = []\n",
        "imagesTraining = []\n",
        "labelsTraining = []\n",
        "classLabels = ['barberry', 'bayberry', 'beach plum', 'bearberry', 'black berry', 'black cherry', 'blueberry', 'ceylon gooseberry', 'chokeberry', 'crowberry']\n",
        "\n",
        "target_size = (224, 224)\n",
        "\n",
        "\n",
        "def readTestingImages(i):\n",
        "    os.chdir('/content/drive/MyDrive/Colab Notebooks/Images/Testing')\n",
        "\n",
        "    for filename in os.listdir():\n",
        "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
        "            # Read Image\n",
        "            image = cv2.imread(filename)\n",
        "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Resize the image\n",
        "            image_resized = cv2.resize(image_rgb, target_size)\n",
        "\n",
        "            # Normalize the image\n",
        "            image_normalized = image_resized / 255.0\n",
        "\n",
        "            if i < 150:\n",
        "              imagesTesting.append(image_normalized)\n",
        "\n",
        "            # Extract label from filename\n",
        "            labels = filename.rsplit(\"_\", 1)\n",
        "            label = labels[0].replace('_', ' ')\n",
        "\n",
        "            # Get the index of the label in classLabels\n",
        "            label_index = classLabels.index(label)\n",
        "\n",
        "            if i < 150:\n",
        "              labelsTesting.append(label_index)\n",
        "\n",
        "            i += 1\n",
        "            if i == 200:\n",
        "              i = 0\n",
        "\n",
        "i = 1\n",
        "def readTrainingImages(i):\n",
        "    os.chdir('/content/drive/MyDrive/Colab Notebooks/Images/Training')\n",
        "\n",
        "    for filename in os.listdir():\n",
        "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
        "            # Read Image\n",
        "            image = cv2.imread(filename)\n",
        "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Resize the image\n",
        "            image_resized = cv2.resize(image_rgb, target_size)\n",
        "\n",
        "            # Normalize the image\n",
        "            image_normalized = image_resized / 255.0\n",
        "\n",
        "            if i < 50:\n",
        "              imagesTraining.append(image_normalized)\n",
        "\n",
        "            # Extract label from filename\n",
        "            labels = filename.rsplit(\"_\", 1)\n",
        "            label = labels[0].replace('_', ' ')\n",
        "\n",
        "            # Get the index of the label in classLabels\n",
        "            label_index = classLabels.index(label)\n",
        "\n",
        "            if i < 50:\n",
        "              labelsTraining.append(label_index)\n",
        "\n",
        "            i += 1\n",
        "            if i == 200:\n",
        "              i = 0\n",
        "\n",
        "readTestingImages(1)\n",
        "readTrainingImages(1)"
      ],
      "metadata": {
        "id": "-oDqT_Y4aSDJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up the CNN architecture\n",
        "\n",
        "We start with a architecture of 7 layers. 2 are convolutional layers. 2 are max-pooling layers, a layer to flatten the previous output into a 1D vector, and 2 connected layers (dense layers).\n",
        "\n",
        "**model = models.Sequential()**: this creates a sequential model, which is a linear stack of layers\n",
        "\n",
        "**model.add(layers.Vonc2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))**: adds a 2D convolutional layer to the model with 32 filters. The number of filters determine the number of channels of the output feature maps. Each filter produces one feature map, and these feature maps collectively capture different aspects of the input data.\n",
        "\n",
        "**model.add(layers.MaxPooling2D((2, 2)))**: adds a max-pooling layer to the model with a pool size of 2x2. Max-pooling reduces the spatial dimensions of the feature maps.\n",
        "\n",
        "**model.add(layers.Flatten())**: this flattens the output of the previous layer into a 1D vector. This reshapes the multi-dimensional feature maps into a 1D vector that can be fed into the fully connected layer. Each element in the vector corresponds to a specific feature or activation in the feature map.\n",
        "\n",
        "**model.add(layers.Dense(64, activation='relu'))**: This adds a fully connected (dense) layer with 64 units and the ReLU activation function.\n",
        "\n",
        "**model.add(layers.Dense(10))**: This adds the final fully connected layer with 10 units. Since this is a classification problem with 10 classes, this layer will produce the output logits for each class."
      ],
      "metadata": {
        "id": "Xy_jTymkhLOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Arquitetura Resnet"
      ],
      "metadata": {
        "id": "6PT1qV-AIbEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_convulational_layer(x, f, k=1, s=1, p='same'):\n",
        "    x = Conv2D(f, k, strides=s, padding=p)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    return MaxPool2D(3, strides=2, padding='same')(x)\n",
        "\n",
        "def identity_block(tensor, f):\n",
        "    x = create_convulational_layer(tensor, f)\n",
        "    x = create_convulational_layer(x, f, 3)\n",
        "    x = Conv2D(4*f, 1)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = Add()([x, tensor])\n",
        "    output = ReLU()(x)\n",
        "    return output\n",
        "\n",
        "def conv_block(tensor, f, s):\n",
        "  x = create_convulational_layer(tensor, f)\n",
        "  x = create_convulational_layer(x, f, 3, s)\n",
        "  x = Conv2D(4*f, 1)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  shortcut = Conv2D(4*f, 1, strides=s)(tensor)\n",
        "  shortcut = BatchNormalization()(shortcut)\n",
        "\n",
        "  x = Add()([x, shortcut])\n",
        "  output = ReLU()(x)\n",
        "  return output\n",
        "\n",
        "def create_resnet_block(x, f, r, s=2):\n",
        "  x = conv_block(x, f, s)\n",
        "  for _ in range(r-1):\n",
        "    x = identity_block(x, f)\n",
        "  return x\n",
        "\n",
        "def build_resnet(input_shape, num_classes):\n",
        "  input = Input(input_shape)\n",
        "\n",
        "  # Primeira camada convolucional adicional\n",
        "  x = create_convulational_layer(input, 64, 3, 2)\n",
        "\n",
        "  # Segunda camada convolucional adicional\n",
        "  x = create_convulational_layer(x, 64, 3, 1)\n",
        "\n",
        "  # Terceira camada convolucional adicional\n",
        "  x = create_convulational_layer(x, 64, 3, 1)\n",
        "\n",
        "  # Quarta camada convolucional adicional\n",
        "  x = create_convulational_layer(x, 64, 3, 1)\n",
        "\n",
        "  # Quinta camada convolucional adicional\n",
        "  x = create_convulational_layer(x, 64, 3, 1)\n",
        "\n",
        "  # Sexta camada convolucional adicional\n",
        "  x = create_convulational_layer(x, 64, 3, 1)\n",
        "\n",
        "  x = create_resnet_block(x, 64, 3, 1)\n",
        "  x = create_resnet_block(x, 128, 4)\n",
        "  x = create_resnet_block(x, 256, 6)\n",
        "  x = create_resnet_block(x, 512, 3)\n",
        "\n",
        "  x = GlobalAvgPool2D()(x)\n",
        "\n",
        "  x = Dropout(0.5)(x)\n",
        "\n",
        "  output = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "  model = Model(input, output)\n",
        "  return model"
      ],
      "metadata": {
        "id": "hZjmPaxA232q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construindo Modelo"
      ],
      "metadata": {
        "id": "fownbo8Njocp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo_resnet_model = Sequential()\n",
        "\n",
        "pretrained_model_for_demo= tf.keras.applications.ResNet50(include_top=False, input_shape=(224,224,3), pooling='avg',classes=10, weights='imagenet')\n",
        "\n",
        "for each_layer in pretrained_model_for_demo.layers:\n",
        "\n",
        "        each_layer.trainable=False\n",
        "\n",
        "demo_resnet_model.add(pretrained_model_for_demo)\n",
        "\n",
        "demo_resnet_model.add(layers.Flatten())\n",
        "\n",
        "demo_resnet_model.add(Dense(512, activation='relu'))\n",
        "\n",
        "demo_resnet_model.add(Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "id": "byy3tUVN29VB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Treinando Modelo"
      ],
      "metadata": {
        "id": "AVvhGAl1KVkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "demo_resnet_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = demo_resnet_model.fit(np.array(imagesTraining), np.array(labelsTraining), epochs=30, validation_data=(np.array(imagesTesting), np.array(labelsTesting)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCB2fKID5lX6",
        "outputId": "5c992954-6f16-49be-cb54-d4b3347cb26e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "16/16 [==============================] - 15s 721ms/step - loss: 2.2376 - accuracy: 0.2380 - val_loss: 2.2572 - val_accuracy: 0.1860\n",
            "Epoch 2/30\n",
            "16/16 [==============================] - 7s 445ms/step - loss: 2.1668 - accuracy: 0.2000 - val_loss: 2.3129 - val_accuracy: 0.1640\n",
            "Epoch 3/30\n",
            "16/16 [==============================] - 7s 443ms/step - loss: 2.1492 - accuracy: 0.2120 - val_loss: 2.2527 - val_accuracy: 0.2127\n",
            "Epoch 4/30\n",
            "16/16 [==============================] - 7s 430ms/step - loss: 2.0901 - accuracy: 0.2500 - val_loss: 2.2732 - val_accuracy: 0.1980\n",
            "Epoch 5/30\n",
            "16/16 [==============================] - 7s 447ms/step - loss: 2.0956 - accuracy: 0.2500 - val_loss: 2.2671 - val_accuracy: 0.1607\n",
            "Epoch 6/30\n",
            "16/16 [==============================] - 7s 432ms/step - loss: 2.1101 - accuracy: 0.2660 - val_loss: 2.3003 - val_accuracy: 0.1420\n",
            "Epoch 7/30\n",
            "16/16 [==============================] - 7s 446ms/step - loss: 2.0976 - accuracy: 0.2500 - val_loss: 2.2746 - val_accuracy: 0.1647\n",
            "Epoch 8/30\n",
            "16/16 [==============================] - 7s 451ms/step - loss: 2.0830 - accuracy: 0.2540 - val_loss: 2.2536 - val_accuracy: 0.1993\n",
            "Epoch 9/30\n",
            "16/16 [==============================] - 7s 444ms/step - loss: 2.0582 - accuracy: 0.2820 - val_loss: 2.2048 - val_accuracy: 0.2133\n",
            "Epoch 10/30\n",
            "16/16 [==============================] - 6s 409ms/step - loss: 2.0538 - accuracy: 0.2760 - val_loss: 2.2310 - val_accuracy: 0.1927\n",
            "Epoch 11/30\n",
            "16/16 [==============================] - 7s 442ms/step - loss: 2.0344 - accuracy: 0.2820 - val_loss: 2.2846 - val_accuracy: 0.1760\n",
            "Epoch 12/30\n",
            "16/16 [==============================] - 6s 405ms/step - loss: 2.1102 - accuracy: 0.2500 - val_loss: 2.2028 - val_accuracy: 0.2093\n",
            "Epoch 13/30\n",
            "16/16 [==============================] - 6s 404ms/step - loss: 2.0695 - accuracy: 0.3040 - val_loss: 2.2354 - val_accuracy: 0.2000\n",
            "Epoch 14/30\n",
            "16/16 [==============================] - 7s 449ms/step - loss: 2.0071 - accuracy: 0.3280 - val_loss: 2.2283 - val_accuracy: 0.1940\n",
            "Epoch 15/30\n",
            "16/16 [==============================] - 6s 402ms/step - loss: 2.0139 - accuracy: 0.3040 - val_loss: 2.2584 - val_accuracy: 0.2067\n",
            "Epoch 16/30\n",
            "16/16 [==============================] - 7s 448ms/step - loss: 2.0212 - accuracy: 0.2980 - val_loss: 2.2727 - val_accuracy: 0.2020\n",
            "Epoch 17/30\n",
            "16/16 [==============================] - 6s 407ms/step - loss: 1.9970 - accuracy: 0.3080 - val_loss: 2.2234 - val_accuracy: 0.2167\n",
            "Epoch 18/30\n",
            "16/16 [==============================] - 6s 413ms/step - loss: 1.9820 - accuracy: 0.3020 - val_loss: 2.3273 - val_accuracy: 0.1693\n",
            "Epoch 19/30\n",
            "16/16 [==============================] - 7s 442ms/step - loss: 1.9836 - accuracy: 0.3040 - val_loss: 2.1601 - val_accuracy: 0.2447\n",
            "Epoch 20/30\n",
            "16/16 [==============================] - 6s 414ms/step - loss: 1.9415 - accuracy: 0.3260 - val_loss: 2.2781 - val_accuracy: 0.2220\n",
            "Epoch 21/30\n",
            "16/16 [==============================] - 7s 443ms/step - loss: 2.0244 - accuracy: 0.2940 - val_loss: 2.1950 - val_accuracy: 0.2220\n",
            "Epoch 22/30\n",
            "16/16 [==============================] - 6s 414ms/step - loss: 1.9522 - accuracy: 0.3160 - val_loss: 2.2746 - val_accuracy: 0.2200\n",
            "Epoch 23/30\n",
            "16/16 [==============================] - 7s 443ms/step - loss: 1.9395 - accuracy: 0.3200 - val_loss: 2.2192 - val_accuracy: 0.2000\n",
            "Epoch 24/30\n",
            "16/16 [==============================] - 6s 411ms/step - loss: 1.9295 - accuracy: 0.3320 - val_loss: 2.2221 - val_accuracy: 0.1847\n",
            "Epoch 25/30\n",
            "16/16 [==============================] - 7s 443ms/step - loss: 1.9000 - accuracy: 0.3400 - val_loss: 2.2450 - val_accuracy: 0.2227\n",
            "Epoch 26/30\n",
            "16/16 [==============================] - 7s 444ms/step - loss: 1.9172 - accuracy: 0.3220 - val_loss: 2.2000 - val_accuracy: 0.2140\n",
            "Epoch 27/30\n",
            "16/16 [==============================] - 6s 409ms/step - loss: 1.8544 - accuracy: 0.3460 - val_loss: 2.2267 - val_accuracy: 0.2007\n",
            "Epoch 28/30\n",
            "16/16 [==============================] - 7s 446ms/step - loss: 1.8661 - accuracy: 0.3540 - val_loss: 2.2125 - val_accuracy: 0.2313\n",
            "Epoch 29/30\n",
            "16/16 [==============================] - 7s 442ms/step - loss: 1.8436 - accuracy: 0.3620 - val_loss: 2.2271 - val_accuracy: 0.2200\n",
            "Epoch 30/30\n",
            "16/16 [==============================] - 6s 405ms/step - loss: 1.8230 - accuracy: 0.3520 - val_loss: 2.1877 - val_accuracy: 0.2560\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Avaliação do Modelo"
      ],
      "metadata": {
        "id": "L6Ny0QuW6ogY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaliação do modelo\n",
        "test_loss, test_acc = model.evaluate(imagesTesting, labelsTesting)\n",
        "print('Acurácia final do modelo: %.2f%%' % (test_acc * 100))"
      ],
      "metadata": {
        "id": "mvlUzRpqlb11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "8c291ef7-27ce-4c90-dd92-2ddeaef991e9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-30583b70d631>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Avaliação do modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimagesTesting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelsTesting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Acurácia final do modelo: %.2f%%'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_acc\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0madapter_cls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m         \u001b[0;31m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1083\u001b[0m             \"Failed to find data adapter that can handle input: {}, {}\".format(\n\u001b[1;32m   1084\u001b[0m                 \u001b[0m_type_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_type_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'numpy.ndarray'>\"}), (<class 'list'> containing values of types {\"<class 'int'>\"})"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.05, 1])\n",
        "plt.legend(loc='lower right')"
      ],
      "metadata": {
        "id": "vUT_636G5Go_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.ylim([0.1, 30])\n",
        "plt.legend(loc='lower right')"
      ],
      "metadata": {
        "id": "MSt_hlal61UI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Salvando Modelo"
      ],
      "metadata": {
        "id": "BedXIqGH4s7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Salvar o modelo\n",
        "model.save('/content/drive/MyDrive/CNN_SOURCE/Modelos/cnn_modelo_resnet.h5')"
      ],
      "metadata": {
        "id": "2HG-Up6iITkp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}